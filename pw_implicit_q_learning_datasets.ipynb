{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CumJu-KrCvU"
      },
      "source": [
        "# Practical Work - Implicit Q-Learning\n",
        "\n",
        "<div class=\"alert alert-info\">\n",
        "\n",
        "...\n",
        "\n",
        "<br>\n",
        "\n",
        "...\n",
        "\n",
        "<br>\n",
        "\n",
        "...\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q37atdWHrCvV"
      },
      "source": [
        "### Imports and auxiliary settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUTCq3LfrCvX",
        "outputId": "5e329b11-6bb9-4048-e189-2af6833ce194"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.125.190.39)] [1 InRele\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.125.190.39)] [Connecti\u001b[0m\r                                                                                                    \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpadcontent.net] [Waiting for\u001b[0m\r                                                                                                    \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [48.6 kB]\n",
            "Get:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,599 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,305 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,326 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,256 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,046 kB]\n",
            "Get:17 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,158 kB]\n",
            "Fetched 8,989 kB in 2s (4,023 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "40 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Collecting swig\n",
            "  Downloading swig-4.1.1.post1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.1.1.post1\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym)\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym)\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym) (4.1.1.post1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2373130 sha256=dcb50c3ca4bc6ad5663a2940b45c548ba7140469efdbc803d325346adbebda33\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.0\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.7/181.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.1.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.1.0)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.0)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.3.post1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=17980ac3327a83f96e1ecbbf19f815bd6f30d4abfc3a7627a433e173f94b7e89\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, shimmy, AutoROM.accept-rom-license, autorom, stable-baselines3\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 farama-notifications-0.0.4 gymnasium-0.29.1 shimmy-1.3.0 stable-baselines3-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!apt update\n",
        "!pip install swig\n",
        "!pip install gym gym[box2d]\n",
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lBwre4vHrCvg"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38rdHq0uTa4L"
      },
      "source": [
        "# Setup Google Drive mount to store your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17KBiSj0TbUb",
        "outputId": "f6b2e280-8fd1-47bb-d972-82828538aa60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "use_google_drive = True\n",
        "if use_google_drive:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM1H78JOz_nh"
      },
      "source": [
        "# Data Collection with Online RL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save dataset to a file\n",
        "def save_dataset(dataset, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(dataset, f)\n",
        "\n",
        "# Function to load dataset\n",
        "def load_dataset(filename):\n",
        "    with open(filename, 'rb') as f:\n",
        "        return pickle.load(f)"
      ],
      "metadata": {
        "id": "Po98St2vBeJz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Enviornment"
      ],
      "metadata": {
        "id": "Ba__mv3JlRPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and wrap the environment\n",
        "env_id = 'LunarLanderContinuous-v2'\n",
        "env = make_vec_env(env_id, n_envs=1)"
      ],
      "metadata": {
        "id": "emqIX214jlnd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Random Dataset"
      ],
      "metadata": {
        "id": "71DGK1Q4N-Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_dataset(env, num_episodes=1000):\n",
        "    tqdm.write('Generating Random Dataset...')\n",
        "    random_dataset = []\n",
        "\n",
        "    for _ in tqdm(range(num_episodes), desc='Random policy steps'):\n",
        "        obs = env.reset()\n",
        "        done = [False]\n",
        "\n",
        "        while not done[0]:\n",
        "            # Sample an action from the action space of the environment\n",
        "            action = [env.action_space.sample()]\n",
        "            # Step through the environment with the action\n",
        "            new_obs, reward, done, _ = env.step(action)\n",
        "            # Append the experience to the dataset\n",
        "            random_dataset.append((obs[0], action[0], reward[0], new_obs[0], done[0]))\n",
        "            obs = new_obs\n",
        "\n",
        "    return random_dataset"
      ],
      "metadata": {
        "id": "9A_sKrjvTOa9"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Random Dataset\n",
        "random_dataset = generate_random_dataset(env, num_episodes=1000)\n",
        "\n",
        "# Save the Dataset\n",
        "save_dataset(random_dataset, 'Random_dataset.pkl')\n",
        "\n",
        "# Move dataset to Google Drive (if using Colab)\n",
        "!cp Random_dataset.pkl /content/drive/MyDrive/Master-AI/Practical-Work\n",
        "\n",
        "tqdm.write('Random_dataset.pkl saved.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiB8TR7EOAo8",
        "outputId": "79f3d81b-d563-4284-eba0-da6fae7747b5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Random Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Random policy steps: 100%|██████████| 1000/1000 [01:03<00:00, 15.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random_dataset.pkl saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Expert Dataset"
      ],
      "metadata": {
        "id": "1A_YdFvSOKR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_expert_dataset(env, model, num_episodes=100):\n",
        "    tqdm.write('Generating Expert Dataset...')\n",
        "    expert_dataset = []\n",
        "\n",
        "    for _ in tqdm(range(num_episodes), desc='Expert policy steps'):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            new_obs, reward, done, info = env.step(action)\n",
        "            # Remove the additional dimension\n",
        "            expert_dataset.append((obs[0], action[0], reward[0], new_obs[0], done[0]))\n",
        "            obs = new_obs\n",
        "\n",
        "    return expert_dataset"
      ],
      "metadata": {
        "id": "BRjqXBxgS4T-"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a model (PPO or DDPG) to convergence\n",
        "expert_model = PPO(\"MlpPolicy\", env, verbose=0)\n",
        "expert_model.learn(total_timesteps=200000)  # Increase timesteps as needed for convergence\n",
        "\n",
        "# Generate Expert Dataset\n",
        "expert_dataset = generate_expert_dataset(env, expert_model, num_episodes=200)\n",
        "\n",
        "# Save the Dataset\n",
        "save_dataset(expert_dataset, 'Expert_dataset.pkl')\n",
        "\n",
        "# Move dataset to Google Drive (if using Colab)\n",
        "!cp Expert_dataset.pkl /content/drive/MyDrive/Master-AI/Practical-Work\n",
        "\n",
        "tqdm.write('Expert_dataset.pkl saved.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn0nz0PKOnz_",
        "outputId": "f2030cd6-c632-40a7-dc10-4ea8c571eac8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Expert Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Expert policy steps: 100%|██████████| 200/200 [05:54<00:00,  1.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expert_dataset.pkl saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Mixed Dataset"
      ],
      "metadata": {
        "id": "kYhilAGSO13l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Random and Expert Datasets\n",
        "random_dataset = load_dataset('Random_dataset.pkl')\n",
        "expert_dataset = load_dataset('Expert_dataset.pkl')\n",
        "\n",
        "tqdm.write('Generating Mixed Dataset...')\n",
        "\n",
        "# Mix datasets: 80% random and 20% expert\n",
        "mixed_dataset = random.sample(random_dataset, int(0.8 * len(random_dataset))) + \\\n",
        "                random.sample(expert_dataset, int(0.2 * len(expert_dataset)))\n",
        "\n",
        "save_dataset(mixed_dataset, 'Mixed_dataset.pkl')\n",
        "\n",
        "# Move dataset to Google Drive (if using Colab)\n",
        "!cp Mixed_dataset.pkl /content/drive/MyDrive/Master-AI/Practical-Work\n",
        "\n",
        "tqdm.write('Mixed_dataset.pkl saved.')"
      ],
      "metadata": {
        "id": "hMPD786uO4MN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2545e92-bf9d-4605-b964-80cee1f99eae"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Mixed Dataset...\n",
            "Mixed_dataset.pkl saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Noisy Dataset"
      ],
      "metadata": {
        "id": "YmQDNV23RIF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_noisy_dataset(env, model, epsilon=0.2, num_episodes=100):\n",
        "    tqdm.write(f'Generating Noisy Dataset...')\n",
        "    noisy_dataset = []\n",
        "\n",
        "    for _ in tqdm(range(num_episodes), desc='Noisy policy steps'):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if np.random.random() < epsilon:\n",
        "                action = [env.action_space.sample()]\n",
        "            else:\n",
        "                action, _ = model.predict(obs, deterministic=True)\n",
        "            new_obs, reward, done, info = env.step(action)\n",
        "            noisy_dataset.append((obs[0], action[0], reward[0], new_obs[0], done[0]))\n",
        "            obs = new_obs\n",
        "\n",
        "    return noisy_dataset"
      ],
      "metadata": {
        "id": "EZ6hEHI3RKRW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Noisy Dataset using the expert model\n",
        "noisy_dataset = generate_noisy_dataset(env, expert_model, num_episodes=200)\n",
        "\n",
        "# Save the Dataset\n",
        "save_dataset(noisy_dataset, 'Noisy_dataset.pkl')\n",
        "\n",
        "# Move dataset to Google Drive (if using Colab)\n",
        "!cp Noisy_dataset.pkl /content/drive/MyDrive/Master-AI/Practical-Work\n",
        "\n",
        "tqdm.write('Noisy_dataset.pkl saved.')"
      ],
      "metadata": {
        "id": "S6ANcFyqSJ4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13aeb36a-b3b2-4405-ef46-9ca0329269ec"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Noisy Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Noisy policy steps: 100%|██████████| 200/200 [07:32<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noisy_dataset.pkl saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Replay Dataset"
      ],
      "metadata": {
        "id": "v9bcejIeSKqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_replay_dataset(env, model, total_timesteps=100000):\n",
        "    tqdm.write(f'Generating Replay Dataset...')\n",
        "    replay_dataset = []\n",
        "    obs = env.reset()\n",
        "\n",
        "    for _ in tqdm(range(total_timesteps), desc='Replay policy steps'):\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        new_obs, reward, done, info = env.step(action)\n",
        "        replay_dataset.append((obs[0], action[0], reward[0], new_obs[0], done[0]))\n",
        "        obs = new_obs if not done[0] else env.reset()\n",
        "\n",
        "    return replay_dataset"
      ],
      "metadata": {
        "id": "Dv_pRbd1SN0F"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Replay Dataset\n",
        "replay_dataset = generate_replay_dataset(env, expert_model, total_timesteps=100000)\n",
        "\n",
        "# Save the Dataset\n",
        "save_dataset(replay_dataset, 'Replay_dataset.pkl')\n",
        "\n",
        "# Move dataset to Google Drive (if using Colab)\n",
        "!cp Replay_dataset.pkl /content/drive/MyDrive/Master-AI/Practical-Work\n",
        "\n",
        "tqdm.write('Replay_dataset.pkl saved.')\n"
      ],
      "metadata": {
        "id": "38BMsSzKUlHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108f6649-07a1-4cab-a39c-e85b07118d78"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating Replay Dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Replay policy steps: 100%|██████████| 100000/100000 [05:27<00:00, 305.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Replay_dataset.pkl saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------------"
      ],
      "metadata": {
        "id": "hGEDRrgZUlZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Close the environments\n",
        "env.close()"
      ],
      "metadata": {
        "id": "LkaZRNq5BrCj"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and check datasets"
      ],
      "metadata": {
        "id": "UZuHSDp-_yri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of dataset filenames\n",
        "dataset_filenames = ['Random_dataset.pkl', 'Expert_dataset.pkl',\n",
        "                     'Mixed_dataset.pkl', 'Noisy_dataset.pkl',\n",
        "                     'Replay_dataset.pkl']\n",
        "\n",
        "# Load and check each dataset\n",
        "for filename in dataset_filenames:\n",
        "    dataset = load_dataset(filename)\n",
        "\n",
        "    print(f\"Checking dataset: {filename}\")\n",
        "    print(f\"Number of entries: {len(dataset)}\")\n",
        "\n",
        "    # Check the first entry\n",
        "    if len(dataset) > 0:\n",
        "        obs, action, reward, new_obs, done = dataset[0]\n",
        "        print(f\"First entry - obs: {obs.shape}, action: {type(action)}, reward: {type(reward)}, new_obs: {new_obs.shape}, done: {type(done)}\")\n",
        "\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzGonVJf_0di",
        "outputId": "3147e42e-8123-4f69-8e53-f4e763ef35ca"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking dataset: Random_dataset.pkl\n",
            "Number of entries: 109894\n",
            "First entry - obs: (8,), action: <class 'numpy.ndarray'>, reward: <class 'numpy.float32'>, new_obs: (8,), done: <class 'numpy.bool_'>\n",
            "\n",
            "\n",
            "Checking dataset: Expert_dataset.pkl\n",
            "Number of entries: 106565\n",
            "First entry - obs: (8,), action: <class 'numpy.ndarray'>, reward: <class 'numpy.float32'>, new_obs: (8,), done: <class 'numpy.bool_'>\n",
            "\n",
            "\n",
            "Checking dataset: Mixed_dataset.pkl\n",
            "Number of entries: 109228\n",
            "First entry - obs: (8,), action: <class 'numpy.ndarray'>, reward: <class 'numpy.float32'>, new_obs: (8,), done: <class 'numpy.bool_'>\n",
            "\n",
            "\n",
            "Checking dataset: Noisy_dataset.pkl\n",
            "Number of entries: 133605\n",
            "First entry - obs: (8,), action: <class 'numpy.ndarray'>, reward: <class 'numpy.float32'>, new_obs: (8,), done: <class 'numpy.bool_'>\n",
            "\n",
            "\n",
            "Checking dataset: Replay_dataset.pkl\n",
            "Number of entries: 100000\n",
            "First entry - obs: (8,), action: <class 'numpy.ndarray'>, reward: <class 'numpy.float32'>, new_obs: (8,), done: <class 'numpy.bool_'>\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}